{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PRGM 13.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Aim: Implement problems on natural language processing - Part of Speech\n",
        "tagging, N-gram using NLTK"
      ],
      "metadata": {
        "id": "dC8zPQwTqRQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Short notes:\n",
        "\n",
        "The **Natural Language Toolkit (NLTK)** is a platform used for building programs for text analysis. One of the more powerful aspects of the NLTK module is the Part of Speech tagging.\n",
        "\n",
        "**Part-of-speech (POS)** tagging is a process of converting a sentence to forms – list of words, list of tuples (where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun, adjective, verb, and so on.\n",
        "\n",
        "**keywords**:\n",
        "\n",
        "\n",
        "Lexicon : Words and their meanings.\n",
        "\n",
        "Token : Each “entity” that is a part of whatever was split up based on rules."
      ],
      "metadata": {
        "id": "vx9m4PmlqT07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tags and their meanings**\n",
        "\n",
        "CD cardinal digit\n",
        "\n",
        "EX existential there (like: “there is” … think of it like “there exists”)\n",
        "\n",
        "FW foreign word\n",
        "\n",
        "IN preposition/subordinating conjunction\n",
        "\n",
        "JJ adjective ‘big’\n",
        "\n",
        "JJR adjective, comparative ‘bigger’\n",
        "\n",
        "JJS adjective, superlative ‘biggest’\n",
        "\n",
        "NN noun, singular ‘desk’\n",
        "\n",
        "NNS noun plural ‘desks’\n",
        "\n",
        "NNP proper noun, singular ‘Harrison’\n",
        "\n",
        "NNPS proper noun, plural ‘Americans’\n",
        "\n",
        "PDT predeterminer ‘all the kids’\n",
        "\n",
        "POS possessive ending parent‘s\n",
        "\n",
        "PRP personal pronoun I, he, she\n",
        "\n",
        "PRP$ possessive pronoun my, his, hers\n",
        "\n",
        "RB adverb very, silently,\n",
        "\n",
        "RBR adverb, comparative better\n",
        "\n",
        "RBS adverb, superlative best\n",
        "\n",
        "RP particle give up\n"
      ],
      "metadata": {
        "id": "IKZAu-oDsT0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N-grams** are continuous sequences of words or symbols or tokens in a document. In technical terms, they can be defined as the neighbouring sequences of items in a document. \n",
        "\n",
        "**Steps for n-gram model:**\n",
        "\n",
        "Explore the dataset\n",
        "\n",
        "Feature extraction\n",
        "\n",
        "Train-test split\n",
        "\n",
        "Basic pre-processing\n",
        "\n",
        "Code to generate N-grams\n",
        "\n",
        "Creating unigrams\n",
        "\n",
        "Creating bigrams\n",
        "\n",
        "Creating trigrams"
      ],
      "metadata": {
        "id": "LwnK2p0VyBBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "  \n",
        "#Dummy text\n",
        "txt = \"Hello. MCA S3 is fantastic. We learn many new concepts and implement them in our practical exams. \"\\\n",
        "\"1st of all the data science is a new paper.\"\n",
        "  \n",
        "# sent_tokenize is one of instances of \n",
        "# PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
        "  \n",
        "tokenized = sent_tokenize(txt)\n",
        "for i in tokenized:\n",
        "      \n",
        "    # Word tokenizers is used to find the words \n",
        "    # and punctuation in a string\n",
        "    wordsList = nltk.word_tokenize(i)\n",
        "  \n",
        "    # removing stop words from wordList\n",
        "    wordsList = [w for w in wordsList if not w in stop_words] \n",
        "  \n",
        "    #  Using a Tagger. Which is part-of-speech \n",
        "    # tagger or POS-tagger. \n",
        "    tagged = nltk.pos_tag(wordsList)\n",
        "  \n",
        "    print(tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpT1pSd5raiN",
        "outputId": "36672450-c3a9-4864-8b70-232a4d4cef0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('Hello', 'NNP'), ('.', '.')]\n",
            "[('MCA', 'NNP'), ('S3', 'NNP'), ('fantastic', 'JJ'), ('.', '.')]\n",
            "[('We', 'PRP'), ('learn', 'VBP'), ('many', 'JJ'), ('new', 'JJ'), ('concepts', 'NNS'), ('implement', 'JJ'), ('practical', 'JJ'), ('exams', 'NN'), ('.', '.')]\n",
            "[('1st', 'CD'), ('data', 'NNS'), ('science', 'NN'), ('new', 'JJ'), ('paper', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-gram model"
      ],
      "metadata": {
        "id": "rq4nRuD-zVMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARA0vZFN230M",
        "outputId": "c09147a3-9415-4b52-f632-9684be5f5b73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#method to generate n-grams:\n",
        "#params:\n",
        "#text-the text for which we have to generate n-grams\n",
        "#ngram-number of grams to be generated from the text(1,2,3,4 etc., default value=1)\n",
        "def generate_N_grams(text,ngram=1):\n",
        "  words=[word for word in text.split(\" \") if word not in set(stopwords.words('english'))]  \n",
        "  print(\"Sentence after removing stopwords:\",words)\n",
        "  temp=zip(*[words[i:] for i in range(0,ngram)])\n",
        "  ans=[' '.join(ngram) for ngram in temp]\n",
        "  return ans"
      ],
      "metadata": {
        "id": "BeHGK5LM26ic"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_N_grams(\"The sun rises in the east\",2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgt3DsHw29Le",
        "outputId": "e3109214-af33-419e-b32c-c4b1a490c6fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence after removing stopwords: ['The', 'sun', 'rises', 'east']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The sun', 'sun rises', 'rises east']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_N_grams(\"The sun rises in the east\",3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMcuc_fn2_Al",
        "outputId": "1d8e4dd3-f30e-4f14-9bbf-c0804295b177"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence after removing stopwords: ['The', 'sun', 'rises', 'east']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The sun rises', 'sun rises east']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_N_grams(\"The sun rises in the east\",4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rM3QGZh2_Hj",
        "outputId": "50d03211-3d3a-46e2-814d-bea7b3ffb552"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence after removing stopwords: ['The', 'sun', 'rises', 'east']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The sun rises east']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_N_grams(\"The sun rises in the east\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQouZlqsDXe5",
        "outputId": "7745cf86-95b0-4910-b569-e4c7a46f6a87"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence after removing stopwords: ['The', 'sun', 'rises', 'east']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'sun', 'rises', 'east']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}